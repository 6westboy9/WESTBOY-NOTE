我们（后端工程师）常见线上问题主要分为两种：业务问题和系统问题。

# 1.业务问题

## 常见表象

1. **接口返回业务错误码**（如 `400xxx`、`BIZ_ERROR`、自定义错误枚举等）
2. **接口返回成功但数据异常**：
    - 查询结果为空（不符合预期）
    - 患者信息、医嘱状态错误
    - 数据重复、丢失或不一致（DB与缓存不一致）
3. **流程卡死/中断**：
    - 医嘱流程停留在某个节点不能进行下去
4. **依赖调用返回业务错误**：
    - 第三方接口返回“参数错误”等业务提示
5. **用户投诉/实施反馈**：
    - 用户无法执行医嘱、扫码没响应
    - 统计报表与实际差异大

特点：**系统本身运行正常**，日志里可能能正常输出，但数据或结果“不对”。

## 定位思路

业务异常表象往往“不像系统异常那么直观”，它不会一下子就挂掉，而是结果“不对”。这类问题更依赖**日志、数据、上下游调用链**来定位。

1. **确认异常现象**
	- 用户投诉/实施反馈的具体表现：用户无法执行医嘱？扫码没响应？
	- 确认是**必现**还是**偶现**。
2. **查看接口返回**
	- 是否返回了业务错误码/提示？
	- 如果接口返回成功但数据不对，需要进一步分析：
	    - 参数传入是否正确？
	    - 数据库结果是否符合预期？
3. **排查应用日志**
	- 搜索请求链路日志（建议结合**traceId**）。
	- 重点关注：
	    - 入参和出参日志。
	    - 业务逻辑中的分支判断。
	    - 抛出的自定义业务异常。
4. **检查数据层**
	- **数据库**：数据是否存在/缺失/脏数据？
	- **缓存**：是否存在缓存未更新/过期/脏读？
	- **消息队列**：消息是否成功投递、被重复消费或丢失？
5. **验证业务流程**
	- 当前状态机/流程引擎是否卡在某节点？
	- 是否存在**幂等问题**（重复提交、重复写入）？
	- 是否有**事务问题**（部分成功、部分失败）？
6. **上下游依赖排查**
	- 调用的第三方接口是否返回业务错误？
	- 是否有参数传递不一致（编码、格式、精度丢失）？
7. **定位根因**
	- 常见根因：
		- 业务代码逻辑缺陷（分支遗漏、边界条件未考虑）。
		- 数据异常（历史数据污染、并发下产生脏数据）。
		- 缓存/DB不一致。
		- 上游参数错误，导致下游逻辑异常。

## 小结

通过多次线上业务异常排查，需要积累了一些经验教训。

* 首先，要重视监控和日志系统的建设，完善的监控和日志能帮助我们及时发现和定位问题。（目前也是迭代式在完善）
* 其次，排查团队的协作至关重要，各成员之间要保持密切沟通，协同作战。（不同开发熟悉的业务领域要善于梳理、总结与分享）
* 再次，要定期对系统进行维护和优化，及时修复潜在的漏洞和问题，提高系统的稳定性和可靠性。(1.大家提出技术优化问题 2.安排技术优化任务)
* 最后，要建立健全的应急预案，在异常发生时能够快速响应和处理，将损失降到最低。​

总之，线上业务异常排查是一项复杂而艰巨的任务，需要我们不断总结经验，完善流程和机制，提高排查效率和解决问题的能力，以保障线上业务的稳定运行，为用户提供更好的服务。

> [!danger] 高效小贴士
>- 业务异常排查的前提在于对业务逻辑及相关代码的深入理解。而要做到这一点，不能仅凭接到产品需求后立即着手开发，更应注重前期的思考与设计。具体来说，应在开发前充分梳理业务流程、明确关键节点和潜在风险，形成初步方案；在开发过程中不断调整和优化方案；之后，通过处理线上问题和持续迭代，进一步加深对业务和实现细节的把握。同时，要善于总结和归纳，逐步构建起自己对业务理解的知识体系。只有这样，才能在面对线上问题时快速定位根源并有效解决。

**个人总结案例**

* [[04.医嘱管理_6.新医嘱&停嘱提醒]]
* [[05.文书管理_6.归档文书]]
* [[06.科室统计.excalidraw]]
* [[07.CA_湛江患者签名.excalidraw]]

# 2.系统问题

## 常见表象

1. **服务不可用**：
    - 接口超时或错误（`500`、`502`、`503`、`504`）
    - 大量请求连接失败（`Connection refused`、`Read timeout`）
2. **性能问题**：
    - 响应时间显著变长
    - QPS/TPS降低，吞吐量下降
3. **资源异常**：
    - CPU飙高、负载过大
    - 内存泄漏/OOM
    - 磁盘写满导致服务卡死
    - 线程池/连接池耗尽
4. **依赖问题**：
    - 数据库连接超时/Too many connections
    - Redis/MQ不可用
    - 外部服务挂掉
5. **部署/配置问题**：
    - 配置错误导致启动失败
    - 环境变量/依赖JAR版本出错

特点：**系统层面报错明显**，通常伴随监控告警、日志报错堆栈。

## 定位思路

首先我们要明白公司内部的技术架构。

![[联新软件服务架构.excalidraw|L|600]]

### 1.服务不可用

**核心目标**：判断是 “服务自身故障” 还是 “流量路由故障”。

- **步骤 1：确认服务实例状态**
    - 检查监控中服务实例的 “存活探针”（如Spring Boot Actuator的`/health`接口）是否正常，若探针失败，说明服务内部已不可用。
    - 登录服务器执行`ps -ef | grep 服务进程名`，确认进程是否存在；若进程存在，用`netstat`查看服务端口是否监听（如`netstat -tunlp | grep 8080`），排除 “进程存活但端口未启动” 的情况。
- **步骤 2：排查错误码对应的责任方**
    - 若报`500`（服务器内部错误）：查看应用日志的错误堆栈，定位具体代码行（如空指针、数据库操作失败），确认是业务逻辑bug还是依赖组件异常。
    - 若报`502`（网关错误）：通常是 “网关无法连接后端服务”，检查后端服务是否存活、网关与服务的网络是否通畅（`telnet 服务IP 端口`）、网关配置的后端服务地址是否正确。
    - 若报`503`（服务暂不可用）：可能服务正在重启。
    - 若报`504`（网关超时）：说明后端服务处理超时，需进一步排查服务性能问题（见 “性能问题定位”）。

### 2.性能问题

**核心目标**：定位 “资源瓶颈” 或 “代码 / 配置低效”。

- **步骤 1：定位性能瓶颈点**
    - 查看服务器监控：
	    - 若CPU使用率 > 80%，可能是计算密集型操作（如复杂循环、序列化）；
	    - 若内存使用率持续上涨，可能是内存泄漏或大对象频繁创建；
	    - 若磁盘IO/wait高，可能是频繁读写本地文件（如日志刷盘）。
    - 查看应用性能监控：通过APM工具（如SkyWalking、Pinpoint）查看 “服务调用链路”，定位哪一环节耗时最长（如数据库操作、外部服务调用、缓存查询）。
- **步骤 2：深入组件排查**
    - 若链路中 “数据库操作耗时高”：查看数据库慢查询日志（如MySQL的`slow_query_log`），分析是否有未走索引的SQL（`explain`执行计划）、表锁 / 行锁竞争（`show processlist`看阻塞线程）。
    - 若 “缓存操作耗时高”：检查Redis监控，是否存在大key（`MEMORY USAGE key`）、频繁过期（`TTL`设置过短）、网络延迟（`ping Redis服务器`）。
    - 若 “应用内部耗时高”：用诊断工具（如Java的Arthas）执行`trace 类名 方法名`查看方法内部耗时，或`thread`命令查看是否有线程阻塞（如`synchronized`锁竞争、IO等待）。
- **步骤 3：验证配置合理性**
    - 检查线程池配置：若核心线程数/最大线程数设置过小，可能导致任务排队（`queueSize`满），查看应用日志是否有 “线程池队列已满” 的告警。
    - 检查连接池配置：是否过小（如低于业务峰值所需连接数，导致请求排队等待连接）；或过大（如超过数据库`max_connections`限制，导致 “Too many connections”）。

### 3.资源异常

**核心目标**：定位 “资源占用源”（进程 / 线程 / 对象）。

- **CPU 飙高**
    1. 执行`top`命令找到 CPU 占用最高的进程（记录 PID）；
    2. 执行`top -Hp PID`找到该进程内 CPU 最高的线程（记录 TID）；
    3. 将TID转为16进制（`printf "%x\n" TID`），用`jstack PID | grep 16进制TID -A 30`查看线程栈，若显示 “RUNNABLE” 且循环逻辑，可能是无限循环；若频繁GC，结合`jstat -gc PID 1000`查看GC频率（如YGC频繁可能是内存分配过快）。
- **内存泄漏 / OOM**
    1. 若报`OutOfMemoryError`，先查看错误类型：`Java heap space`是堆内存不足，`Metaspace`是元空间不足；
    2. 用`jmap -dump:format=b,file=heap.dump PID`抓取堆快照（OOM 时可通过`-XX:+HeapDumpOnOutOfMemoryError`自动生成）；
    3. 用MAT工具分析快照，查看“Dominator Tree”，定位哪些对象（如大List、缓存Map）占用内存且未释放，结合代码确认是否存在对象引用未回收（如静态集合未清理）。
- **线程池 / 连接池耗尽**
    1. 查看应用日志，是否有“Thread pool is exhausted”“Connection pool is full” 等报错；
    2. 检查线程池 / 连接池配置（如`corePoolSize`/`maxPoolSize`、`maxConnections`），对比监控中的 “当前活跃数”，若接近最大值，说明配置过小或任务/连接未及时释放；
    3. 用`jstack`查看线程状态，若大量线程处于 “WAITING”（如等待锁、等待资源），可能是下游服务响应慢导致连接/线程阻塞。

### 4.依赖问题

**核心目标**：判断是 “依赖组件自身故障” 还是 “应用与依赖的交互故障”。

- **数据库相关（连接超时 / Too many connections）**
    1. 检查数据库监控：用`show status like 'Threads_connected'`查看当前连接数，对比`max_connections`，若接近则是数据库连接数不足；
    2. 查看应用数据库连接池配置（若`maximumPoolSize` > 数据库`max_connections`，会导致 “Too many connections”，需调小连接池上限；
    3. 若连接超时，用`telnet 数据库IP 端口`验证网络通畅性，用`show processlist`查看数据库是否有慢查询阻塞（导致连接无法释放）。
- **Redis/MQ 不可用**
    1. 检查组件自身状态：Redis用`redis-cli ping`验证存活，MQ（如RabbitMQ）用管理界面查看节点状态、队列堆积情况；
    2. 若频繁断连，查看网络监控（如丢包率`ping -c 100 RedisIP`），或组件是否触发限流（如Redis的`maxclients`限制）。
- **外部服务挂掉**
    1. 用`curl 外部服务接口`直接调用，验证是否返回错误（如503）；
    2. 查看应用中外部服务的调用配置：是否超时时间过短（导致“Read timeout”）、重试策略不合理（重试加剧服务压力）；

### 5.部署/配置问题

**核心目标**：定位 “配置/环境/依赖不匹配” 的具体点。

- **配置错误导致启动失败**
    1. 查看服务启动日志（如`nohup.out`），重点关注 “启动失败” 关键词（如 “Invalid config”“Missing required property”）；
    2. 对比配置文件（如`application.yml`）与文档，检查是否有拼写错误（如`server.port`写成`server.prot`）、必填项缺失（如数据库URL未配置）；
    3. 验证配置注入是否正确：若用Spring Boot，可通过`@Value`打印配置值，确认是否与预期一致（如环境变量覆盖了配置文件）。
- **依赖 JAR 版本 / 环境变量错误**
    1. 查看启动日志中的 “ClassNotFoundException”“NoSuchMethodError”，通常是JAR版本冲突；
    2. 用`mvn dependency:tree`（Maven）或`gradle dependencies`（Gradle）分析依赖树，排除冲突版本（通过`exclusion`）；
    3. 检查环境变量：确认是否有遗漏（如`JAVA_HOME`未设置）、值错误（如生产环境用了测试数据库配置）。

## 小结

大家可以看到上述内容过于八股，更重要的在于实操，系统问题的定位和解决很考验技术功底。

> [!danger] 高效定位
> 1. **理解系统架构**：**画图**！厘清服务关系、数据流向、依赖组件，故障时能快速推断影响范围。
> 2. **善用监控告警**：建立**层层递进**的监控（基础设施 → 应用服务 → 业务指标），告警信息常是排查起点。
> 3. **日志规范与集中**：应用日志应**规范输出**（如JSON格式）、包含关键信息（TraceID、用户ID），并收集到**日志中心**（ELK/Loki）方便检索和关联。
> 4. **掌握核心命令**：熟练使用`top`, `vmstat`, `iostat`, `netstat`, `ss`, `strace`, `tcpdump`等**基础命令**，在无GUI环境的服务器上它们是救命稻草。
> 5. **保持好奇心与记录**：对遇到的奇怪问题**保持好奇**，排查后**记录复盘**，形成自己的“知识库”，经验是最好的工具。

# 3.总结

上面主要阐述了具体的问题和定位思路，那么开发人员实际该如何应对线上故障呢？

[开发人员该如何应对线上故障](https://developer.aliyun.com/article/860631)

1. 及时汇报、协作排查
2. 稳定第一，快速止损
3. 清扫战场，及时复盘自我检讨

[联新软件部门-历史线上重大事故](https://lachesis-mh.feishu.cn/wiki/HvzYwtRJliEpRbk2YeYcj7amnmb)
